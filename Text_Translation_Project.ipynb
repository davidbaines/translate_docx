{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidbaines/translate_docx/blob/main/Text_Translation_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This Notebook shows a full pipeline for Text language identification and Translation using Facebook models fasttext and No Language Left Behind (NLLB). \n",
        "\n",
        "First, we start with taking an input text in any language, then we will detect its language code using fasttext.\n",
        "\n",
        "After that, we take the entered text, and predicted label and feed them to NLLB which translates text from our original language to whatever language NLLB supports. \n",
        "\n",
        "Source: https://colab.research.google.com/drive/1fsbzykS5ANEMVcn7gtp8Wl7gkmRzbwOW\n",
        "\n",
        "Webpage: https://medium.com/mlearning-ai/text-translation-using-nllb-and-huggingface-tutorial-7e789e0f7816"
      ],
      "metadata": {
        "id": "35FOlWbhTYGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ulRccc5acJxm",
        "outputId": "3c61f175-ded9-420f-83f7-04644a97b108",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the language model pretrained file\n",
        "!wget https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin"
      ],
      "metadata": {
        "id": "Eft9k_IQGs4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca773985-b551-4aac-c807-39c9408c657c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-29 21:15:11--  https://dl.fbaipublicfiles.com/nllb/lid/lid218e.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1176355829 (1.1G) [application/octet-stream]\n",
            "Saving to: ‘lid218e.bin.1’\n",
            "\n",
            "lid218e.bin.1       100%[===================>]   1.09G  28.7MB/s    in 35s     \n",
            "\n",
            "2023-01-29 21:15:47 (31.9 MB/s) - ‘lid218e.bin.1’ saved [1176355829/1176355829]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and functions"
      ],
      "metadata": {
        "id": "UyZgNOJfQRZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install fasttext\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "REkIwyNbe94B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80c3c32-f22d-49b4-a4cd-1f022c8aa3fa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.8/dist-packages (0.8.11)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from python-docx) (4.9.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.8/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.8/dist-packages (from fasttext) (2.10.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test sentence tokenizer:\n",
        "para = \"Hello World. It's good to see you. Thanks for buying this book.\"\n",
        "sent_tokenize(para)"
      ],
      "metadata": {
        "id": "cjvUpkEEX8tP",
        "outputId": "3dc32e55-de1f-48ff-ccbf-78373d7985bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "pretrained_lang_model = \"/content/lid218e.bin\" # path of pretrained model file\n",
        "model = fasttext.load_model(pretrained_lang_model)"
      ],
      "metadata": {
        "id": "VjxcmlAYfAzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77549724-1c86-469f-c764-e909edf6344b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets enter a test text in the original language, here we will translate from Arabic to Spanish."
      ],
      "metadata": {
        "id": "vNCxBqEnTOf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"صباح الخير، الجو جميل اليوم والسماء صافية.\""
      ],
      "metadata": {
        "id": "U7nrW7uXQpi-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(text, k=1) \n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRhYAikeQr11",
        "outputId": "81d96848-6297-4807-946a-1840ed4a7578"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(('__label__arb_Arab',), array([0.99960977]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang = predictions[0][0].replace('__label__', '')"
      ],
      "metadata": {
        "id": "GvF0TBVlWFQA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports and Functions"
      ],
      "metadata": {
        "id": "VA791wYjVxVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Translation"
      ],
      "metadata": {
        "id": "fmTB8nw2QO_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Csu1xOnJXxPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df807151-3114-41ab-fe5d-5ededcfb9355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.3.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U pip transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "mYc2Jgjtj2Ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f07b4d-e076-4e3f-ce21-3c590aa6d35c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Smallest 600M parameter model - distilled\n",
        "checkpoint = 'facebook/nllb-200-distilled-600M'\n",
        "\n",
        "# Medium 1.3B parameter model - distilled\n",
        "# checkpoint = 'facebook/nllb-200-distilled-1.3B'\n",
        "\n",
        "# Medium 1.3B parameter model\n",
        "# 1.3B parameter model\n",
        "\n",
        "# Large 3.3B parameter model\n",
        "# checkpoint = 'facebook/nllb-200-3.3B'\n",
        "\n"
      ],
      "metadata": {
        "id": "IVvNqR8XRAbQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "gEErPO8NX_-_"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test translation with model."
      ],
      "metadata": {
        "id": "KNC8MNnVbd3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_lang = 'spa_Latn'\n",
        "translation_pipeline = pipeline('translation', \n",
        "                                model=model, \n",
        "                                tokenizer=tokenizer, \n",
        "                                src_lang=input_lang, \n",
        "                                tgt_lang=target_lang, \n",
        "                                max_length = 400)\n",
        "output = translation_pipeline(text)\n",
        "print(output[0]['translation_text'])"
      ],
      "metadata": {
        "id": "4ZiJOolWjHrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812c5302-0677-4059-c608-c7f6527ffc8a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buenos días, el clima es hermoso y el cielo está limpio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import docx\n",
        "from docx import Document\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from os.path import join, isfile\n",
        "\n",
        "\n",
        "def get_paragraphs_from_docx(file):\n",
        "        \n",
        "    paras = []\n",
        "    # Open connection to Word Document\n",
        "    doc = docx.Document(file)\n",
        "    \n",
        "    # read in each paragraph in file and store the style name with it.\n",
        "    for p, para in enumerate(doc.paragraphs,1):\n",
        "        this_para = {'style': para.style.name}\n",
        "        sentences = []\n",
        "        for i, sentence in enumerate(sent_tokenize(para.text),1):\n",
        "            sentences.append(sentence)\n",
        "        this_para['sentences'] = sentences\n",
        "        paras.append(this_para)\n",
        "\n",
        "    #print(f'Found {len(styles_in_doc)} styles {styles_in_doc} in this document.')    \n",
        "    return paras\n",
        "\n",
        "\n",
        "def put_sentences_to_docx(sentences,file_in, file_out):\n",
        "    # Sentences should be a dictionary of sent\n",
        "    paras = []\n",
        "\n",
        "    # Open connection to Word Document\n",
        "    doc = docx.Document(file_out)\n",
        "    \n",
        "    # Output the paragraphs and sentences with the correct style.\n",
        "#    for sentence in sentences:\n",
        "\n",
        "\n",
        "def translate_sentences(file_in, input_lang, output_lang, file_out):\n",
        "\n",
        "        sentences = get_sentences_from_docx(input_file)\n",
        "        for sentence in sentences:\n",
        "            print(sentence)\n",
        "\n",
        "        print(f\"Found {len(sentences)} sentences in {input_file.resolve}\")\n",
        "\n",
        "# From https://github.com/henrihapponen/docxedit/blob/main/docxedit.py\n",
        "\n",
        "\n",
        "def replace_string(doc: object, old_string: str, new_string: str):\n",
        "    \"\"\"\n",
        "    Replaces an old string (placeholder) with a new string\n",
        "    without changing the formatting of the text.\n",
        "    Args:\n",
        "        doc (Object): The docx document object.\n",
        "        old_string (String): The old string to replace.\n",
        "        new_string (String): The new string to replace the old one with.\n",
        "    Returns:\n",
        "        modified document\n",
        "    \"\"\"\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        if old_string in paragraph.text:\n",
        "            inline = paragraph.runs\n",
        "            print(f\"Inline is {inline}\")\n",
        "            \n",
        "            for i in range(len(inline)):\n",
        "                if old_string in inline[i].text:\n",
        "                    text = inline[i].text.replace(str(old_string), str(new_string))\n",
        "                    inline[i].text = text\n",
        "                    return True\n",
        "    \n",
        "    # Couldn't fild old_string. Print to help find out why:\n",
        "    \n",
        "    return False\n",
        "\n",
        "\n",
        "def copy_tree(source, destination):\n",
        "    shutil.copytree(source, destination, symlinks=False, ignore=None, dirs_exist_ok=True)\n"
      ],
      "metadata": {
        "id": "iSLc8FP5V25v"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process one to test:\n",
        "input_lang = 'eng_Latn'\n",
        "output_lang = 'tpi_Latn'\n",
        "\n",
        "input_folder = Path(\"drive/MyDrive/EIL-Mark\")\n",
        "output_folder = Path(\"drive/MyDrive/EIL-Mark-Tagalog\")\n",
        "ext_in = 'docx'\n",
        "ext_out = 'docx'\n",
        "\n",
        "# Copy the input files to the output directory work only on the copies.\n",
        "copy_tree(input_folder, output_folder)\n",
        "\n",
        "\n",
        "translation_pipeline = pipeline('translation', \n",
        "                        model=model, \n",
        "                        tokenizer=tokenizer, \n",
        "                        src_lang=input_lang, \n",
        "                        tgt_lang=output_lang, \n",
        "                        max_length = 400)"
      ],
      "metadata": {
        "id": "tqucbrVJdfzE"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of copied files.\n",
        "output_files = [file for file in output_folder.rglob(\"*.\" + ext_in)]\n",
        "print(f\"Found {len(output_files)} {ext_in} files in {output_folder.resolve()}\")\n",
        "\n",
        "# Process one to test:\n",
        "for output_file in output_files[:2]:\n",
        "    \n",
        "    output_file = output_file.resolve()\n",
        "    print(f\"Initial output_file is {output_file}\")\n",
        "\n",
        "    # Skip files that have already been translated.\n",
        "    #if output_file.with_suffix(\".translated.docx\").is_file():\n",
        "    #    continue\n",
        "\n",
        "    # Open the output as a document\n",
        "    document = Document(output_file)\n",
        "\n",
        "    # Save it with a new name.\n",
        "    translated_file = output_file.with_suffix(\".translated.docx\").resolve()\n",
        "    document.save(translated_file)\n",
        "\n",
        "    paragraphs_in = get_paragraphs_from_docx(output_file)\n",
        "    #paragraphs_out = paragraphs_in\n",
        "\n",
        "    for para, paragraph_in in enumerate(paragraphs_in):\n",
        "        if not paragraph_in['sentences'] :\n",
        "            continue\n",
        "        else :\n",
        "            for sent, sentence in enumerate(paragraph_in['sentences']):\n",
        "                translated_sentence = translation_pipeline(sentence)[0]['translation_text']\n",
        "                #translated_sentence = output[0]['translation_text']\n",
        "                if not replace_string(document, sentence, translated_sentence):\n",
        "                    print(f\"Couldn't replace \\n{sentence}\\n  with:\\n{translated_sentence}\\n in file:\\n{translated_file}\")\n",
        "                else: \n",
        "                    print(f\"{sentence}        ->         {translated_sentence}\")\n",
        "                    document.save(translated_file)\n",
        "\n",
        "    document.save(translated_file)\n",
        "    print(f\"Wrote translated file to {translated_file}\")\n",
        "\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ4QcD8ugyrn",
        "outputId": "5550b93e-f000-4e64-a137-743d9acc071f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 323 docx files in /content/drive/MyDrive/EIL-Mark-Tagalog\n",
            "Initial output_file is /content/drive/MyDrive/EIL-Mark-Tagalog/0.intro/Mark Introduction 1.docx\n",
            "Inline is [<docx.text.run.Run object at 0x7f8da97324f0>]\n",
            "Mark Introduction        ->         Toktok Bilong Mak\n",
            "Inline is [<docx.text.run.Run object at 0x7f8da9732eb0>]\n",
            "Part 1: The author’s and audience’s story        ->         Hap 1: Stori bilong ol man i raitim ol dispela stori na stori bilong ol manmeri\n",
            "Inline is [<docx.text.run.Run object at 0x7f8da9732e50>, <docx.text.run.Run object at 0x7f8da9863850>]\n",
            "You may introduce the book of Mark through story form in as concrete a way as possible.        ->         Yu ken stori long buk Mak long rot bilong stori long rot i stret.\n",
            "Inline is [<docx.text.run.Run object at 0x7f8da9732e50>, <docx.text.run.Run object at 0x7f8daace8fd0>]\n",
            "One way to do this is to tell the following story and have your translation team act it out.        ->         Wanpela rot bilong mekim olsem em long storiim stori i kamap bihain na tokim ol lain bilong tanim tok long mekim olsem.\n",
            "Wrote translated file to /content/drive/MyDrive/EIL-Mark-Tagalog/0.intro/Mark Introduction 1.translated.docx\n",
            "Initial output_file is /content/drive/MyDrive/EIL-Mark-Tagalog/0.intro/Mark Introduction 1.translated.docx\n",
            "Inline is [<docx.text.run.Run object at 0x7f8da8e4fe20>]\n",
            "Toktok Bilong Mak        ->         Toktok Bilong Mak\n",
            "Inline is [<docx.text.run.Run object at 0x7f8da8e4fe50>]\n",
            "Hap 1: Stori bilong ol man i raitim ol dispela stori na stori bilong ol manmeri        ->         Hap 1: Stori bilong ol man i bin raitim ol dispela stori na stori bilong ol manmeri\n",
            "Inline is [<docx.text.run.Run object at 0x7f8da9438b20>, <docx.text.run.Run object at 0x7f8da9438eb0>]\n",
            "Yu ken stori long buk Mak long rot bilong stori long rot i stret.        ->         Yu ken stori long buk Mak long rot bilong stori long rot i stret.\n",
            "Inline is [<docx.text.run.Run object at 0x7f8da8e44400>, <docx.text.run.Run object at 0x7f8da8e44160>]\n",
            "Wanpela rot bilong mekim olsem em long storiim stori i kamap bihain na tokim ol lain bilong tanim tok long mekim olsem.        ->         One rot bilong mekim olsem em long storiim stori i kamap bihain na tokim ol narapela bilong tanim tok long mekim olsem.\n",
            "Wrote translated file to /content/drive/MyDrive/EIL-Mark-Tagalog/0.intro/Mark Introduction 1.translated.translated.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Create the output doument\n",
        "    document = docx.Document(output_file)\n",
        "\n",
        "    paragraphs_in = get_paragraphs_from_docx(input_file)\n",
        "    #paragraphs_out = paragraphs_in\n",
        "\n",
        "    for para, paragraph_in in enumerate(paragraphs_in):\n",
        "        if not paragraph_in['sentences'] :\n",
        "            continue\n",
        "        else :\n",
        "            for sentence in paragraph_in['sentences']:\n",
        "                translation_pipeline = pipeline('translation', \n",
        "                                    model=model, \n",
        "                                    tokenizer=tokenizer, \n",
        "                                    src_lang=input_lang, \n",
        "                                    tgt_lang=output_lang, \n",
        "                                    max_length = 400)\n",
        "                \n",
        "                translated_sentence = translation_pipeline(sentence)[0]['translation_text']\n",
        "                #translated_sentence = output[0]['translation_text']\n",
        "                if not replace_string(document, sentence, translated_sentence):\n",
        "                    print(f\"Couldn't replace \\n{sentence}\\n  with:\\n{translated_sentence}\\n in file:\\n{output_file.resolve()}\")\n",
        "                    \n",
        "\n",
        "    document.save(output_file.resolve())\n",
        "    print(f\"Wrote translated file to {output_file.resolve()}\")\n"
      ],
      "metadata": {
        "id": "VDanOwijgdmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "    translated_paras = []\n",
        "    for paragraph in paragraphs[:3]:\n",
        "        translated_paras.append(paragraph[0])\n",
        "        if len(paragraph) == 1:\n",
        "            # This paragraph only contains style info and no text.\n",
        "            translations.append(translate)\n",
        "            continue\n",
        "        elif len(paragraph)  >1 :\n",
        "            for sentence in paragraph:\n",
        "                translation_pipeline = pipeline('translation', \n",
        "                                    model=model, \n",
        "                                    tokenizer=tokenizer, \n",
        "                                    src_lang=input_lang, \n",
        "                                    tgt_lang=output_lang, \n",
        "                                    max_length = 400)\n",
        "                \n",
        "                output = translation_pipeline(sentence)\n",
        "                translated_sentence = output[0]['translation_text']\n",
        "\n",
        "                translated_sentences.append(translated_sentence)\n",
        "                print(f\"source: {sentence}\\nTarges: {translated_sentence}\")\n",
        "\n",
        "        translations.append(translated_sentences)\n",
        "        \n",
        "for translated_para in translated_paras:\n",
        "    print(translated_para)"
      ],
      "metadata": {
        "id": "kSsrM8mNw89R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QC7ymKoRdy79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = [r'You may introduce the book of Mark through story form in as concrete a way as possible. ',\n",
        "               r'One way to do this is to tell the following story and have your translation team act it out. ',\n",
        "               r'Ahead of time, choose the following characters: Mark, Peter, Jesus, Paul, and Barnabas. ',\n",
        "               r'The rest of the team can play the parts of the followers and believers. ',\n",
        "               r'Choose parts of the room to represent different parts of the world: Jerusalem, Antioch, Cyprus, and Rome. ',\n",
        "               r'These four places could be the four corners of the room. ',\n",
        "               r'As you tell the story, the characters in that part of the story can walk to the part of the room that is representing the place in the story.',\n",
        "               r'Have the characters act out the journeys and the actions as you tell the story.',\n",
        "               r'You may have the team retell the story after you tell it, again acting it out.',\n",
        "               r'Help them if they forget parts of the story.',]      "
      ],
      "metadata": {
        "id": "5_N4OXWOuvl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outputs = []\n",
        "max_length = max(len(input_text) for input_text in input_texts)\n",
        "print(f\"Max length is {max_length}\")\n",
        "\n",
        "for input_text in input_texts:\n",
        "    translation_pipeline = pipeline('translation', \n",
        "                                model=model, \n",
        "                                tokenizer=tokenizer, \n",
        "                                src_lang=input_lang, \n",
        "                                tgt_lang=output_lang, \n",
        "                                max_length = 400)\n",
        "\n",
        "    outputs.append(translation_pipeline(input_text))\n",
        "\n",
        "for output in outputs:\n",
        "    print(output[0]['translation_text'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oRxLK-dwWtt",
        "outputId": "bdbf92be-5c58-4637-cb04-439435550853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length is 141\n",
            "Yu ken kamapim buk Mak long rot bilong stori long rot i stret tru. \n",
            "Wanpela rot bilong mekim olsem em long storiim stori i kamap bihain na tokim ol lain bilong tanim tok long mekim olsem. \n",
            "Taim yu laik kisim sampela hap tok, makim ol dispela man: Mak, Pita, Jisas, Pol, na Barnabas. \n",
            "Ol narapela insait long lain inap mekim wok bilong ol disaipel na ol bilipman. \n",
            "Pinis long makim ol hap bilong rum bilong makim ol narapela hap bilong graun: Jerusalem, Antiok, Saiprus, na Rom. \n",
            "Dispela 4-pela hap inap makim 4-pela kona bilong rum.\n",
            "Taim yu stori, ol man i stap long dispela hap stori i ken wokabaut i go long hap bilong rum em ples i stap long stori.\n",
            "Taim yu stori, yu mas tokim ol man long ol samting yu mekim long rot bilong raun na wokabaut.\n",
            "Ating bai yu tokim lain long stori gen taim yu stori pinis, na bihain bai yu mekim olsem.\n",
            "Sapos ol i lusim tingting long sampela hap stori, orait helpim ol.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tpi_output_texts = []\n",
        "for output in outputs:\n",
        "    tpi_output_texts.append(output[0]['translation_text'])"
      ],
      "metadata": {
        "id": "Os7BoPu5x2P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang = 'tpi_Latn'\n",
        "output_lang = 'eng_Latn'\n",
        "\n",
        "tpi_input_texts = tpi_output_texts\n",
        "eng_outputs = []\n",
        "eng_output_texts = []\n",
        "\n",
        "for input_text in tpi_input_texts:\n",
        "    translation_pipeline = pipeline('translation', \n",
        "                                model=model, \n",
        "                                tokenizer=tokenizer, \n",
        "                                src_lang=input_lang, \n",
        "                                tgt_lang=output_lang, \n",
        "                                max_length = 400)\n",
        "\n",
        "    eng_outputs.append(translation_pipeline(input_text))\n",
        "\n",
        "eng_output_texts = [eng_output[0]['translation_text'] for eng_output in eng_outputs]"
      ],
      "metadata": {
        "id": "k-pLsM10ySwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for eng_output_text in eng_output_texts:\n",
        "    print(eng_output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js7EHqDpx93J",
        "outputId": "495f7bfc-8fd7-4984-8e13-4d6cec2c6c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can make Mark's book sound by telling the truth.\n",
            "And the way of the translator is to make a report, and the way of the translator is to make a report.\n",
            "Now if you want to take part in the discussion, choose these men: Mark, Peter, Jesus, Paul and Barnabas.\n",
            "The rest of the congregation can share in the ministry of the disciples and the faithful.\n",
            "And the city of Jerusalem, and the city of Antioch, and the city of Cyprus, and the city of Rome, were chosen.\n",
            "And the four corners of the house were four corners.\n",
            "When you tell the story, the people in the story can walk to the room where the story is told.\n",
            "When thou speakest, thou shalt speak thy words, and thy ways.\n",
            "And thou shalt speak, and thou shalt speak, and thou shalt speak.\n",
            "If they forget a thing, help them.\n"
          ]
        }
      ]
    }
  ]
}